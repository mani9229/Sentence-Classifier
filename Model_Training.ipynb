{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_2tVZu8hON0"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVduqPZwXMav"
      },
      "source": [
        "Importing the Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qT4NXxLXJlz",
        "outputId": "e00e5233-b044-4b25-c60c-b960310d8431"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "nltk.download('stopwords')\n",
        "# Using the stopwords.\n",
        "from nltk.corpus import stopwords\n",
        "# Initialize the stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.externals import joblib\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "PdO54nlfYTZn",
        "outputId": "38d5955b-02ca-4309-e467-9c035fc5b87f"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d4d8a789-3db9-447d-b4e2-82bfd0c295a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d4d8a789-3db9-447d-b4e2-82bfd0c295a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Context.csv to Context.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OdRZ00AWYZl4",
        "outputId": "9f9ab643-5e7c-4852-80b4-1df5b58422e3"
      },
      "source": [
        "#Reading the file\n",
        "nlptag = pd.read_csv('Context.csv')\n",
        "nlptag.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Context/Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The eternal mystique of Goldman Sachs</td>\n",
              "      <td>Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Either you don't care enough to actually tell ...</td>\n",
              "      <td>Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am such an IDIOT.</td>\n",
              "      <td>Heavy Emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>While lifting weights on Friday and doing bent...</td>\n",
              "      <td>Health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Something's watching me</td>\n",
              "      <td>Animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Context/Topic\n",
              "0              The eternal mystique of Goldman Sachs       Politics\n",
              "1  Either you don't care enough to actually tell ...           Love\n",
              "2                                I am such an IDIOT.  Heavy Emotion\n",
              "3  While lifting weights on Friday and doing bent...         Health\n",
              "4                            Something's watching me        Animals"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjYzEhPBYrt6",
        "outputId": "483b2a63-fe54-4e1b-c9a0-c1d697dc49bd"
      },
      "source": [
        "#Renaming for the convenience\n",
        "nlptag.rename(columns={\"Context/Topic\": \"Topic\"} , inplace = True)\n",
        "nlptag['Topic']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             Politics\n",
              "1                 Love\n",
              "2        Heavy Emotion\n",
              "3               Health\n",
              "4              Animals\n",
              "             ...      \n",
              "31381    Heavy Emotion\n",
              "31382    Heavy Emotion\n",
              "31383        Education\n",
              "31384         Politics\n",
              "31385         Politics\n",
              "Name: Topic, Length: 31386, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjgy3c7ZSdf"
      },
      "source": [
        "**Text cleaning and preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6IYzt4J5Zc-w",
        "outputId": "d841034b-d5c0-47c2-8b0d-1ceb1e4e8d3e"
      },
      "source": [
        "##cleaning\n",
        "nlptag['Cleaned'] = nlptag['Text'].apply(lambda x: \"\".join(x.lower() for x in str ((x.split()))))\n",
        "nlptag['Cleaned'] = nlptag['Cleaned'].str.replace(r\"[^a-zA-Z ]+\",\" \").replace('\\s+', ' ',regex=True)\n",
        "nlptag.head(5)\n",
        "                                                                              "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The eternal mystique of Goldman Sachs</td>\n",
              "      <td>Politics</td>\n",
              "      <td>the eternal mystique of goldman sachs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Either you don't care enough to actually tell ...</td>\n",
              "      <td>Love</td>\n",
              "      <td>either you don t care enough to actually tell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am such an IDIOT.</td>\n",
              "      <td>Heavy Emotion</td>\n",
              "      <td>i am such an idiot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>While lifting weights on Friday and doing bent...</td>\n",
              "      <td>Health</td>\n",
              "      <td>while lifting weights on friday and doing ben...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Something's watching me</td>\n",
              "      <td>Animals</td>\n",
              "      <td>something s watching me</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...                                            Cleaned\n",
              "0              The eternal mystique of Goldman Sachs  ...             the eternal mystique of goldman sachs \n",
              "1  Either you don't care enough to actually tell ...  ...   either you don t care enough to actually tell...\n",
              "2                                I am such an IDIOT.  ...                                i am such an idiot \n",
              "3  While lifting weights on Friday and doing bent...  ...   while lifting weights on friday and doing ben...\n",
              "4                            Something's watching me  ...                           something s watching me \n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7krmX5ObZpsC",
        "outputId": "eb13370e-7ed5-4515-c80c-4d4eabdfc83c"
      },
      "source": [
        "##Removing stopwords and lemmenting\n",
        "stop = stopwords.words(\"english\")\n",
        "nlptag.Cleaned = nlptag.Cleaned.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlptag.Cleaned=nlptag.Cleaned.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "nlptag['Cleaned_lemmatized']=nlptag.Cleaned.apply(lambda x:\"\".join([lemmatizer.lemmatize(y) for y in x]))\n",
        "nlptag.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Cleaned</th>\n",
              "      <th>Cleaned_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The eternal mystique of Goldman Sachs</td>\n",
              "      <td>Politics</td>\n",
              "      <td>eternal mystique goldman sachs</td>\n",
              "      <td>eternal mystique goldman sachs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Either you don't care enough to actually tell ...</td>\n",
              "      <td>Love</td>\n",
              "      <td>either care enough actually tell u feel dumbas...</td>\n",
              "      <td>either care enough actually tell u feel dumbas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am such an IDIOT.</td>\n",
              "      <td>Heavy Emotion</td>\n",
              "      <td>idiot</td>\n",
              "      <td>idiot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>While lifting weights on Friday and doing bent...</td>\n",
              "      <td>Health</td>\n",
              "      <td>lifting weights friday bent rows felt sharp pa...</td>\n",
              "      <td>lifting weights friday bent rows felt sharp pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Something's watching me</td>\n",
              "      <td>Animals</td>\n",
              "      <td>something watching</td>\n",
              "      <td>something watching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...                                 Cleaned_lemmatized\n",
              "0              The eternal mystique of Goldman Sachs  ...                     eternal mystique goldman sachs\n",
              "1  Either you don't care enough to actually tell ...  ...  either care enough actually tell u feel dumbas...\n",
              "2                                I am such an IDIOT.  ...                                              idiot\n",
              "3  While lifting weights on Friday and doing bent...  ...  lifting weights friday bent rows felt sharp pa...\n",
              "4                            Something's watching me  ...                                 something watching\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeklW1uCZzJr"
      },
      "source": [
        "#function for other cleaning\n",
        "def cleansingLayer(line):\n",
        "  try:\n",
        "    line = line.lower().strip()\n",
        "    line = re.sub(r'\\s+', ' ',line)\n",
        "    line = re.sub(r'thanks.*', '',line)\n",
        "    line = re.sub(r\"what's\", \"what is\",line)\n",
        "    line = re.sub(r\"don't\", \"do not\",line)\n",
        "    line = re.sub(r\"should've\", \"should have\",line)\n",
        "    line = re.sub(r\"couldn't\", \"could not\",line)\n",
        "    line = re.sub(r\"can't\", \"can not\",line)\n",
        "    line = re.sub(r\"aren't\", \"are not\",line)\n",
        "    line = re.sub(r\"didn't\", \"did not\",line)\n",
        "    line = re.sub(r\"doesn't\",\"does not\",line)\n",
        "    line = re.sub(r\"hadn't\", \"had not\",line)\n",
        "    line = re.sub(r\"hasn't\",\"has not\",line)\n",
        "    line = re.sub(r\"haven't\",\"have not\",line)\n",
        "    line = re.sub(r\"isn't\", \"is not\",line)\n",
        "    line = re.sub(r\"mightn't\",\"might not\",line)\n",
        "    line = re.sub(r\"mustn't\", \"must not\",line)\n",
        "    line = re.sub(r\"shouldn't\", \"should not\",line)\n",
        "    line = re.sub(r\"wasn't\", \"was not\",line)\n",
        "    line = re.sub(r\"weren't\",\"were not\",line)\n",
        "    line = re.sub(r\"wouldn't\",\"would not\",line)\n",
        "    line = re.sub(r\"\\'ve\", \" have \",line)\n",
        "    line = re.sub(r\"n't\", \"not\",line)\n",
        "    line = re.sub(r\"\\'re\", \" are \",line)\n",
        "    line = re.sub(r\"\\'d\",\" would\",line)\n",
        "    line = re.sub(r\"\\'ll\",\" will \",line)\n",
        "    line = re.sub(r\" abt \",\" about \",line)\n",
        "    line = re.sub(r\"n ot\", \" not \",line)\n",
        "    lem = WordNetLemmatizer()\n",
        "    line = ' '.join(lem.lemmatize(i, pos='v') for i in line.split())\n",
        "    line = re.sub('[^a-zA-Z ]', ' ', line)\n",
        "    line = re.sub(r'\\s+', ' ',line)\n",
        "    line = line.strip()\n",
        "\n",
        "  except Exception as e:\n",
        "     print(e)\n",
        "     print(line)\n",
        "     print('++++')\n",
        "\n",
        "  return line   \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPEMDTk8Z8oX",
        "outputId": "fb399bab-ee6c-467a-e646-8fb8d589a03d"
      },
      "source": [
        "nlptag = nlptag.drop_duplicates(['Cleaned','Topic'])\n",
        "nlptag.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30421, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvt-bEQOaCJo",
        "outputId": "9214c330-5470-44d7-e3e6-08224588f7a3"
      },
      "source": [
        "nlptag['Topic'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Heavy Emotion    3571\n",
              "Religion         3363\n",
              "Love             3106\n",
              "Self             3028\n",
              "Compliment       2784\n",
              "Animals          2570\n",
              "Health           2562\n",
              "Education        2431\n",
              "Joke             2429\n",
              "Science          2395\n",
              "Politics         2182\n",
              "Name: Topic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WIqaKDdaL9P"
      },
      "source": [
        "nlptag['Cleaned_lemmatized'] = nlptag['Cleaned_lemmatized'].apply(lambda x: cleansingLayer(str(x)))\n",
        "nlptag['Cleaned_lemmatized'].replace('',np.nan, inplace=True)\n",
        "nlptag.dropna(subset=['Cleaned_lemmatized'], inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ziQJnrV8aWpB",
        "outputId": "ed01a989-d845-493e-f24c-042226fd55f2"
      },
      "source": [
        "nlptag"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Cleaned</th>\n",
              "      <th>Cleaned_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The eternal mystique of Goldman Sachs</td>\n",
              "      <td>Politics</td>\n",
              "      <td>eternal mystique goldman sachs</td>\n",
              "      <td>eternal mystique goldman sachs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Either you don't care enough to actually tell ...</td>\n",
              "      <td>Love</td>\n",
              "      <td>either care enough actually tell u feel dumbas...</td>\n",
              "      <td>either care enough actually tell u feel dumbas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am such an IDIOT.</td>\n",
              "      <td>Heavy Emotion</td>\n",
              "      <td>idiot</td>\n",
              "      <td>idiot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>While lifting weights on Friday and doing bent...</td>\n",
              "      <td>Health</td>\n",
              "      <td>lifting weights friday bent rows felt sharp pa...</td>\n",
              "      <td>lift weight friday bend row felt sharp pain lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Something's watching me</td>\n",
              "      <td>Animals</td>\n",
              "      <td>something watching</td>\n",
              "      <td>something watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31380</th>\n",
              "      <td>4 types of jews as reported by an Israeli scholar</td>\n",
              "      <td>Religion</td>\n",
              "      <td>types jews reported israeli scholar</td>\n",
              "      <td>type jews report israeli scholar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31382</th>\n",
              "      <td>Looks are everything</td>\n",
              "      <td>Heavy Emotion</td>\n",
              "      <td>looks everything</td>\n",
              "      <td>look everything</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31383</th>\n",
              "      <td>Teacher: write a thing that you want to do til...</td>\n",
              "      <td>Education</td>\n",
              "      <td>teacher write thing want till</td>\n",
              "      <td>teacher write thing want till</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31384</th>\n",
              "      <td>General Secretary of India's ruling party, the...</td>\n",
              "      <td>Politics</td>\n",
              "      <td>general secretary india ruling party bjp threa...</td>\n",
              "      <td>general secretary india rule party bjp threate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31385</th>\n",
              "      <td>Heastie rewarded contributors, racked up unusu...</td>\n",
              "      <td>Politics</td>\n",
              "      <td>heastie rewarded contributors racked unusual e...</td>\n",
              "      <td>heastie reward contributors rack unusual expense</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30312 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  ...                                 Cleaned_lemmatized\n",
              "0                  The eternal mystique of Goldman Sachs  ...                     eternal mystique goldman sachs\n",
              "1      Either you don't care enough to actually tell ...  ...  either care enough actually tell u feel dumbas...\n",
              "2                                    I am such an IDIOT.  ...                                              idiot\n",
              "3      While lifting weights on Friday and doing bent...  ...  lift weight friday bend row felt sharp pain lo...\n",
              "4                                Something's watching me  ...                                    something watch\n",
              "...                                                  ...  ...                                                ...\n",
              "31380  4 types of jews as reported by an Israeli scholar  ...                   type jews report israeli scholar\n",
              "31382                               Looks are everything  ...                                    look everything\n",
              "31383  Teacher: write a thing that you want to do til...  ...                      teacher write thing want till\n",
              "31384  General Secretary of India's ruling party, the...  ...  general secretary india rule party bjp threate...\n",
              "31385  Heastie rewarded contributors, racked up unusu...  ...   heastie reward contributors rack unusual expense\n",
              "\n",
              "[30312 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vmZYytiac5C"
      },
      "source": [
        "X= nlptag['Cleaned_lemmatized']\n",
        "y = nlptag['Topic']\n",
        "x_train, x_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yj1MUDubjSc"
      },
      "source": [
        "**Vectorizing the data using TFIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzdTR3gLbqm1",
        "outputId": "0e5e17bb-ecb8-4f44-af86-895dda3223a5"
      },
      "source": [
        "tfid_text = TfidfVectorizer(ngram_range= (1,3),max_features=3149)\n",
        "tfid_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=3149,\n",
              "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjoURJZ2cCmi"
      },
      "source": [
        "#Stroing the as the idf object\n",
        "train_text_pkl = tfid_text.fit(x_train)\n",
        "train_text = tfid_text.fit_transform(x_train)\n",
        "test_text = tfid_text.transform(x_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZ7O2GHcaGb"
      },
      "source": [
        "#Saving it as a pickle to use for the feature tests\n",
        "feature_name = tfid_text.get_feature_names()\n",
        "joblib.dump(train_text_pkl,open(\"binary_object_stream\",\"wb\"))\n",
        "with open('binary_object_stream','rb') as f:\n",
        "  f_content = f.read()\n",
        "tf.io.gfile.GFile('tfidf.pkl','wb').write(f_content)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC8G-RD6csB2"
      },
      "source": [
        "f_content = tf.io.gfile.GFile('tfidf.pkl','rb').read()\n",
        "with open('binary_object_stream','wb') as f:\n",
        "  f.write(f_content)\n",
        "tfidftest_sb = joblib.load(open(\"binary_object_stream\",\"rb\"))\n",
        "\n",
        "tf1_new = TfidfVectorizer(ngram_range= (1,3),max_features=3149,vocabulary= tfidftest_sb.vocabulary_)\n",
        "tdidftest = tf1_new.transform(x_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_VCk45hhUY7"
      },
      "source": [
        "tdidftest = tf1_new.transform(x_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phT7VLVbdB4h",
        "outputId": "81c4dda6-6d3b-422f-ebac-f811ece20c81"
      },
      "source": [
        "test_text.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3032, 3149)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DazBi0DadGMz",
        "outputId": "100b15b5-4636-4cd2-fdf1-1353857586a9"
      },
      "source": [
        "tdidftest.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3032, 3149)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKdS1kg0dJlM",
        "outputId": "1ab70999-ebc7-48bb-dd36-963e8c19baec"
      },
      "source": [
        "train_text.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27280, 3149)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaVqEqiAdX82",
        "outputId": "3bbe0879-3128-4957-f355-68275995fdfb"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(random_state=0)\n",
        "#X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
        "model.fit(train_text, y_train)\n",
        "y_pred = model.predict(test_text)\n",
        "y_pred_proba = model.predict_proba(test_text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPXZ9s_ndt2g",
        "outputId": "9dcbfab7-b69f-4c2a-deea-e34934221683"
      },
      "source": [
        "print(\"Accuracy score (training: {0:.3f}\".format(model.score(train_text,y_train)))\n",
        "print(\"Accuracy score(test): {0:.3f}\".format(model.score(test_text,y_test)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score (training: 0.712\n",
            "Accuracy score(test): 0.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KK8WG6Wd490",
        "outputId": "67a92a00-77d6-430d-a150-3b9dd0f5a5ed"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "logistic_pred=model.predict(test_text)\n",
        "logistic_pred_train = model.predict(train_text)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(classification_report(y_train,logistic_pred_train))\n",
        "print(classification_report(y_test,logistic_pred))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Animals       0.77      0.73      0.75      2341\n",
            "   Compliment       0.78      0.70      0.74      2460\n",
            "    Education       0.76      0.72      0.74      2181\n",
            "       Health       0.72      0.70      0.71      2284\n",
            "Heavy Emotion       0.63      0.68      0.66      3202\n",
            "         Joke       0.76      0.75      0.76      2177\n",
            "         Love       0.70      0.69      0.69      2788\n",
            "     Politics       0.87      0.81      0.84      1967\n",
            "     Religion       0.79      0.75      0.77      3028\n",
            "      Science       0.69      0.65      0.67      2151\n",
            "         Self       0.52      0.66      0.58      2701\n",
            "\n",
            "     accuracy                           0.71     27280\n",
            "    macro avg       0.73      0.71      0.72     27280\n",
            " weighted avg       0.72      0.71      0.71     27280\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Animals       0.63      0.64      0.63       226\n",
            "   Compliment       0.72      0.66      0.69       269\n",
            "    Education       0.66      0.55      0.60       243\n",
            "       Health       0.57      0.58      0.58       272\n",
            "Heavy Emotion       0.51      0.56      0.53       358\n",
            "         Joke       0.68      0.68      0.68       249\n",
            "         Love       0.60      0.58      0.59       311\n",
            "     Politics       0.82      0.77      0.80       214\n",
            "     Religion       0.71      0.69      0.70       327\n",
            "      Science       0.57      0.52      0.55       242\n",
            "         Self       0.45      0.55      0.49       321\n",
            "\n",
            "     accuracy                           0.61      3032\n",
            "    macro avg       0.63      0.62      0.62      3032\n",
            " weighted avg       0.62      0.61      0.61      3032\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnvcySiOidKp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Gb2WoJigj-"
      },
      "source": [
        "## Saving model to pkl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM5JNTA9imhL"
      },
      "source": [
        "#Saving it as a pickle to use for the feature tests\n",
        "\n",
        "joblib.dump(model,open(\"binary_object_stream\",\"wb\"))\n",
        "with open('binary_object_stream','rb') as f:\n",
        "  f_content = f.read()\n",
        "tf.io.gfile.GFile('model.pkl','wb').write(f_content)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4e-5n-4Kz7mB",
        "outputId": "cd22aae0-ecbc-4675-ff4b-7e3f5c80667b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('tfidf.pkl')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bb5de66c-33f9-4401-ab9e-0527f5b90b8a\", \"tfidf.pkl\", 13696809)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}